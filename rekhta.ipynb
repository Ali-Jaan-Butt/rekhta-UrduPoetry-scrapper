{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9f2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy import Selector\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d58c203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of poet: syed suleman gilani\n",
      "Enter the file name: gilani\n"
     ]
    }
   ],
   "source": [
    "name = input('Enter the name of poet: ').split()\n",
    "file = input('Enter the file name: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f44ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghazal_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b1e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(name)==3:\n",
    "    res = requests.get('https://www.rekhta.org/poets/'+name[0]+'-'+name[1]+'-'+name[2]+'?lang=ur')\n",
    "else:\n",
    "    res = requests.get('https://www.rekhta.org/poets/'+name[0]+'-'+name[1]+'?lang=ur')\n",
    "sel = Selector(text=res.text)\n",
    "full = sel.xpath('//div[@class=\"readFullBgBtn\"]/a/@href').extract_first()\n",
    "if full!=None:\n",
    "    res = requests.get(full)\n",
    "    sel = Selector(text=res.text)\n",
    "    for i in sel.xpath('//div[@class=\"contentListItems nwPoetListBody\"]/a/@href').extract():\n",
    "        ghazal_url.append(i)\n",
    "    load_more = sel.xpath('//div[@class=\"contentLoadMore\"]/div/@data-url').extract()\n",
    "    if load_more!=[]:\n",
    "        load_more_url =['https://www.rekhta.org'+i for i in load_more]\n",
    "        for j in load_more_url:\n",
    "            res = requests.get(j)\n",
    "            sel = Selector(text=res.text)\n",
    "            for k in sel.xpath('//div[@class=\"contentListItems nwPoetListBody\"]/a/@href').extract():\n",
    "                ghazal_url.append(k)\n",
    "    else:\n",
    "        for k in sel.xpath('//div[@class=\"contentListItems nwPoetListBody\"]/a/@href').extract():\n",
    "            ghazal_url.append(k)\n",
    "else:\n",
    "    for k in sel.xpath('//div[@class=\"contentListItems nwPoetListBody\"]/a/@href').extract():\n",
    "        ghazal_url.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3112e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = {'Shyr':[],'Name':[],'Emotion 1':[],'Emotion 2':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51071cb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    "for gh in tqdm(ghazal_url):\n",
    "    res = requests.get(gh)\n",
    "    sel = Selector(text=res.text)\n",
    "    poet_name = sel.xpath('//a[@class=\"ghazalAuthor\"]/text()').extract_first()\n",
    "    for l in sel.xpath('//div[@class=\"w\"]'):\n",
    "        one = l.xpath('./div/p/span/text()').extract()\n",
    "        one = ' '.join(one)\n",
    "        lines['Shyr'].append(one)\n",
    "        lines['Name'].append(poet_name)\n",
    "        lines['Emotion 1'].append(None)\n",
    "        lines['Emotion 2'].append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83455f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lines)\n",
    "df.to_csv(file+'.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164ae2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_urdu_text(txt):\n",
    "    urdu_characters = [u'\\u0600', u'\\u06FF']\n",
    "    urdu_count = sum(1 for char in txt if char >= urdu_characters[0] and char <= urdu_characters[1])\n",
    "    return urdu_count / len(txt) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdee67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english_rows(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['Is_Urdu'] = df['Shyr'].apply(is_urdu_text)\n",
    "    df = df[df['Is_Urdu'] == True]\n",
    "    df.drop(columns=['Is_Urdu'], inplace=True)\n",
    "    df.to_csv(file+'.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a01f5491",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_english_rows(file+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
